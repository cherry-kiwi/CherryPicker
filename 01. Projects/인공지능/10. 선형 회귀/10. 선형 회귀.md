# 선형 회귀
- 지도 학습
	- 회귀(regression): 연속적인 값 예측
		- ex) 주택 면적 고려하여 주택 가격 예측하는 것
	- 분류(classification): 입력값을 카테고리로 분류
		- ex) 주어진 이미지가 고양이인지 강아지인지 판별
![[Pasted image 20231014174753.png]]

## 회귀
- 데이터들을 2차원 공간에 찍은 후, 데이터를 잘 설명할 수 있는 직선 or 곡선을 찾는 문제
- $y=f(x)$ 에서 출력 y가 실수고, 입력 x도 실수 일 때 함수 $f(x)$ 를 예측하는 것
- ex) 주식 가격 예측, 온도 변화, 전력 수요 변동
## 선형 회귀
- 직선 모델을 사용하여 회귀 문제 푸는 것
- 부모의 키와 자녀의 키의 관계, 공부 시간과 학점과의 관계,  CPU 속도와 프로그램 실행 시간 예측
### 선형 회귀 소개
$$f(x) = mx + b$$
- 입력 데이터를 가장 잘 설명하는 직선의 기울기($m$)와 절편($b$)을 찾는 문제
- 우리가 제어할 수 있는 값: 기울기($m$), 절편($b$)
- 머신러닝에서는 기울기($m$) = 가중치($w$) / 절편($b$) = 바이어스($b$)
$$f(x) = wx + b$$
### 선형 회귀 종류
1. 단순 선형 회귀: 독립 변수($x$)가 하나인 선형 회귀
$$f(x) = wx + b$$
- $x, y$는 학습 데이터 / $f(x)$는 우리의 예측
2. 다중 선형 회귀
$$f(x,y,z) = w_0 + w_1x + w_2y + w_3z$$
- $w_0, w_1x, w_2y, w_3z$는 계수 or 가중치. 모델이 학습하려고 하는 매개 변수

### 선형 회귀 원리
![[Pasted image 20231016212753.png]]
- 선형 회귀에선 이 기울기 중 학습 데이터와 가장 잘 맞는 하나의 직선 선택하게 됨
- 가장 잘 맞는 직선: 데이터와 직선의 간격이 작을수록 잘 맞음
	- 간격은 음수일 수 있으니 제곱을 하는게 좋음
	- 간격의 제곱을 모두 합한 값 = 손실 함수(loss function) 혹은 비용 함수(cost function)
- 훈련 데이터 세트가 ($x_1, y_1$), ($x_2, y_2$), ($x_3, y_3$) 일 때, 손실은 다음과 같이 표현
- $$Loss=\frac{1}{3}((f(x_1)-y_1)^2+(f(x_2)-y_2)^2+(f(x_3)-y_3)^2$$
- 또는 $$Loss(w,b)=\frac{1}{n}\sum^{n}_{i=1}(f(x_i)-y)^2=\frac{1}{n}\sum^{n}_{i=1}(f(wx_i+b)-y_i)^2$$
- 여기서 $n$은 훈련 데이터의 개수
- 학습은 손실 함수값을 최소로 하는 $w$와 $b$를 찾는 것
- $$\underset{w, b}{\operatorname{argmin}}Loss(w,b)$$
- Loss() 함수를 최소로 하는 $w$와 $b$를 계산한다

## 선형 회귀에서 손실 함수 최소화 방법
### 분석적인 방법
- 직선의 기울기와 절편 계산하기
### 경사하강법 (Gradient Descent Method)
- 현재 위치에서의 경사(기울기)를 이용하여 방향 잡는 방법
- 손실 함수를 줄이기 위해선 기울기의 반대 방향으로 이동

|  기울기(-)  |  기울기(+)  |
|:-----------:|:-----------:|
| 이동방향(+) | 이동방향(-) |
- 이렇게 기울기의 반대 방향으로 가다 보면 최저값을 찾을 수 있음

### 선형 회귀에서 경사하강법
- 손실: 모든 데이터 포인트의 손실을 제곱하여 합한 후에 평균을 낸 값
- 손실 함수: 
- $$Loss(w,b)=\frac{1}{n}\sum^{n}_{n=1}(f(x_i)-y_i)^2=\frac{1}{n}\sum^{n}_{n=1}(f(wx_i+b)-y_i)^2$$
- $f(x_i)$: 모델이 예측한 값 / $y_i$: 실제 값
1. $w, b$를 모두 0으로 설정, 학습률(한 번에 기울기를 변경하는 양) = 0.01
2. 손실 함수를 $w$에 대해 편미분$$\frac{\partial Loss(w,b)}{\partial w}=\frac{1}{n}\sum^{n}_{i=1}2((wx_i+b)-y_i)(x_i)=\frac{2}{n}\sum^{n}_{i=1}x_i((wx_i+b)-y_i)$$
3. 손실 함수를 $b$에 대해 미분$$\frac{\partial Loss(w,b)}{\partial w}=\frac{2}{n}\sum^{n}_{i=1}2((wx_i+b)-y_i)(1)=\frac{2}{n}\sum^{n}_{i=1}x_i((wx_i+b)-y_i)$$
4. $w, b$를 다음과 같이 업데이트$$w=w-0.01\times\frac{\partial Loss}{\partial w}$$ $$b=b-0.01\times\frac{\partial Loss}{\partial b}$$
5. 이 과정을 손실 함수의 값이 아주 작아질 때까지 반복

### 선형 회귀 파이썬 구현
```
X = np.array([])

```